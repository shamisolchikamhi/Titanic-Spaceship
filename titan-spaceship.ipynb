{
 "cells": [
  {
   "cell_type": "code",
   "id": "25ebba22",
   "metadata": {
    "papermill": {
     "duration": 0.002801,
     "end_time": "2023-11-15T07:14:46.413152",
     "exception": false,
     "start_time": "2023-11-15T07:14:46.410351",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-14T09:29:14.299158Z",
     "start_time": "2024-10-14T09:28:41.357221Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2ce5987e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-15T07:14:46.419898Z",
     "iopub.status.busy": "2023-11-15T07:14:46.419393Z",
     "iopub.status.idle": "2023-11-15T07:14:46.944806Z",
     "shell.execute_reply": "2023-11-15T07:14:46.942860Z"
    },
    "papermill": {
     "duration": 0.532378,
     "end_time": "2023-11-15T07:14:46.947930",
     "exception": false,
     "start_time": "2023-11-15T07:14:46.415552",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-14T09:29:14.308116Z",
     "start_time": "2024-10-14T09:29:14.301148Z"
    }
   },
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check missing data",
   "id": "2e890bb2446a84e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:06.343147Z",
     "start_time": "2024-10-13T12:25:06.304079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_target_balance(data, target_column):\n",
    "    \"\"\"\n",
    "    Check the balance of the target variable.\n",
    "    \"\"\"\n",
    "    target_balance = data[target_colum].value_counts(normalize=True)\n",
    "    return target_balance\n",
    "\n",
    "def check_missing_values(data):\n",
    "    \"\"\"\n",
    "    Check for missing values in the dataset.\n",
    "    \"\"\"\n",
    "    missing_values = data.isnull().sum()\n",
    "    return missing_values[missing_values > 0]\n",
    "\n",
    "def fill_missing_values(data):\n",
    "    \"\"\"\n",
    "    Fill missing values in the dataset.\n",
    "    - Drop rows with missing values in critical columns.\n",
    "    - Fill categorical columns with the mode.\n",
    "    - Fill numerical columns with the median.\n",
    "    \"\"\"\n",
    " \n",
    "    # Fill missing categorical values with the mode\n",
    "    for column in ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name']:\n",
    "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "\n",
    "    # Fill missing numerical values with the median\n",
    "    for column in ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n",
    "        data[column].fillna(data[column].median(), inplace=True)\n",
    "\n",
    "    return data\n"
   ],
   "id": "74a950d3cd73d58b",
   "execution_count": 77,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "KNN imputing",
   "id": "192464ca912e4f00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fill_missing_values_with_knn(data, categorical_columns, numerical_columns, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Impute missing values for both categorical and numerical variables using KNN Imputation.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The dataframe with missing values.\n",
    "    - categorical_columns: List of categorical column names.\n",
    "    - numerical_columns: List of numerical column names.\n",
    "    - n_neighbors: Number of neighbors to use for KNN imputation (default is 5).\n",
    "    \n",
    "    Returns:\n",
    "    - data: The dataframe with imputed missing values.\n",
    "    \"\"\"\n",
    "    # Create a copy of the data to avoid altering the original dataset\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Temporarily encode categorical variables using LabelEncoder\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        data_copy[col] = le.fit_transform(data_copy[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Apply KNNImputer for both categorical and numerical columns\n",
    "    knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_data = pd.DataFrame(knn_imputer.fit_transform(data_copy), columns=data_copy.columns)\n",
    "\n",
    "    # Reverse the encoding for categorical variables\n",
    "    for col in categorical_columns:\n",
    "        imputed_data[col] = imputed_data[col].round().astype(int)\n",
    "        imputed_data[col] = label_encoders[col].inverse_transform(imputed_data[col])\n",
    "\n",
    "    # Return the imputed dataset\n",
    "    return imputed_data"
   ],
   "id": "aab2f3b86b315add",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Standadise data",
   "id": "140581e8ef859a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:06.920945Z",
     "start_time": "2024-10-13T12:25:06.913967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standardize_numerical_columns(data):\n",
    "    \"\"\"\n",
    "    Standardize numerical columns in the dataset using z-score normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The dataframe containing the data.\n",
    "    - numerical_columns: List of numerical column names to be standardized.\n",
    "    \n",
    "    Returns:\n",
    "    - data: The dataframe with standardized numerical columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the numerical columns that you want to standardize\n",
    "    numerical_columns = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    # Create a copy of the data to avoid altering the original dataset\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Standardize the numerical columns\n",
    "    data_copy[numerical_columns] = scaler.fit_transform(data_copy[numerical_columns])\n",
    "\n",
    "    return data_copy"
   ],
   "id": "9116a887d9ad0af5",
   "execution_count": 78,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Engineering",
   "id": "7653e5528c993f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:07.565979Z",
     "start_time": "2024-10-13T12:25:07.550373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the dataset.\n",
    "    - Split 'Cabin' into 'Deck', 'CabinNumber', and 'Side'.\n",
    "    - Create 'TotalSpend' feature.\n",
    "    - Create 'AgeGroup' feature.\n",
    "    \"\"\"\n",
    "    # Split 'Cabin' into 'Deck', 'CabinNumber', and 'Side'\n",
    "    data[['Deck', 'CabinNumber', 'Side']] = data['Cabin'].astype(str).str.split('/', expand=True)\n",
    "    data['CabinNumber'] = pd.to_numeric(data['CabinNumber'], errors='coerce')\n",
    "    # data = data.drop(columns=['Cabin'])\n",
    "\n",
    "    # Create 'TotalSpend' feature\n",
    "    data['TotalSpend'] = (\n",
    "        data['RoomService'] +\n",
    "        data['FoodCourt'] +\n",
    "        data['ShoppingMall'] +\n",
    "        data['Spa'] +\n",
    "        data['VRDeck']\n",
    "    )\n",
    "\n",
    "    # Create age groups\n",
    "    bins = [0, 12, 18, 35, 60, 100]\n",
    "    labels = ['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n",
    "    data['AgeGroup'] = pd.cut(data['Age'], bins=bins, labels=labels)\n",
    "    \n",
    "    # Ensure 'Name' column contains strings and handle missing values\n",
    "    data['Name'] = data['Name'].astype(str).fillna('Unknown Unknown')\n",
    "    \n",
    "    # Split 'Name' into a list of strings\n",
    "    name_split = data['Name'].str.split(' ', n=1, expand=True)  # Split only at the first space\n",
    "    \n",
    "    # Assign the first part as 'FirstName' and the second part as 'Surname'\n",
    "    data['FirstName'] = name_split[0]\n",
    "    data['Surname'] = name_split[1].fillna('Unknown')\n",
    "\n",
    "    return data"
   ],
   "id": "459412624132ac1b",
   "execution_count": 79,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Categorical variables encoding",
   "id": "d2fa06557cf65c25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:08.090180Z",
     "start_time": "2024-10-13T12:25:08.079783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_categorical_features(data):\n",
    "    \"\"\"\n",
    "    Encode categorical features using LabelEncoder.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Name', 'Deck', 'Side', \n",
    "                   'AgeGroup', 'FirstName', 'Surname', 'Cabin']:\n",
    "        data[column] = label_encoder.fit_transform(data[column].astype(str))\n",
    "    \n",
    "    return data"
   ],
   "id": "8400848cc5e27f9d",
   "execution_count": 80,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocess data ",
   "id": "e3a803ef4d1343ef"
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_data(data, target_column='Transported'):\n",
    "    \"\"\"\n",
    "    Perform the full preprocessing pipeline on the dataset.\n",
    "    \"\"\"\n",
    "    data = fill_missing_values(data)\n",
    "\n",
    "    # Step 2: Perform feature engineering\n",
    "    data = feature_engineering(data)\n",
    "        # Apply the function to standardize numerical columns\n",
    "    data = standardize_numerical_columns(data)\n",
    "    # Step 3: Encode categorical variables\n",
    "    data = encode_categorical_features(data)\n",
    "\n",
    "    # Step 4: Check target variable balance\n",
    "    target_balance = check_target_balance(data, target_column)\n",
    "    print(\"Target Balance:\\n\", target_balance)\n",
    "\n",
    "    return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:08.915770Z",
     "start_time": "2024-10-13T12:25:08.907686Z"
    }
   },
   "id": "92c8e97290e957a8",
   "execution_count": 81,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preditions",
   "id": "ea55df2abeeddc51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:09.737790Z",
     "start_time": "2024-10-13T12:25:09.722412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prediction_results(model, data_path='test.csv', output_path='predictions.csv'):\n",
    "    \"\"\"\n",
    "    Process the dataset, make predictions using the provided model, and save the results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_path: Path to the dataset to be processed.\n",
    "    - model: Trained machine learning model to use for predictions.\n",
    "    - output_path: Path to save the prediction results.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['Transported'] = False\n",
    "    data = preprocess_data(data)\n",
    "    passenger_ids = data['PassengerId']\n",
    "    data = data.drop(columns=['PassengerId', 'Transported', 'Name', 'VIP', 'FirstName'])\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # Ensure predictions are a 2D array\n",
    "    if predictions.ndim == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    \n",
    "    # Convert float predictions to 'True' or 'False' if needed\n",
    "    if predictions.dtype == np.float32:\n",
    "        predictions = np.where(predictions > 0.5, 'True', 'False')\n",
    "    # Create a DataFrame with PassengerId and Transported predictions\n",
    "    output = pd.DataFrame({'PassengerId': passenger_ids, 'Transported': predictions.flatten()})\n",
    "    output.to_csv(output_path, index=False)\n",
    "    \n",
    "    # return output\n"
   ],
   "id": "f15ef8d57c51bcff",
   "execution_count": 82,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data ",
   "id": "c3d0afdf8d6fd91d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:25:11.797012Z",
     "start_time": "2024-10-13T12:25:11.134099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spaceship_data = pd.read_csv('train.csv')\n",
    "spaceship_data.head()\n",
    "\n",
    "check_missing_values(spaceship_data)\n",
    "spaceship_data = preprocess_data(spaceship_data)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = spaceship_data.drop(columns=['PassengerId', 'Transported', 'Name', 'VIP', 'FirstName'])\n",
    "y = spaceship_data['Transported']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "31cdb07b70015b62",
   "execution_count": 83,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "1. Random Forrest"
   ],
   "id": "f12d5774a9db8b62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T06:49:34.269490Z",
     "start_time": "2024-10-12T06:45:26.143225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_model_rf = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model_rf.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "report_best = classification_report(y_test, y_pred_best)\n",
    "best_params, accuracy_best, report_best\n",
    "\n",
    "# featjure importance \n",
    "# Extract feature importances\n",
    "importances = best_model_rf.feature_importances_\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns.tolist(),\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
    "plt.show()\n",
    "\n",
    "prediction_results(model=best_model_rf)"
   ],
   "id": "1875957eb247e066",
   "execution_count": 74,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Ensemble methods (Stacking), improved from 80% to 80.36%",
   "id": "d8e29abed5695341"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T06:41:57.746406Z",
     "start_time": "2024-10-12T06:41:41.855381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# 2. XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "# 3. Stacking Classifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=3)\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "report_stack = classification_report(y_test, y_pred_stack)\n",
    "\n",
    "# Results\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n",
    "print(report_gb)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "print(report_xgb)\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_stack)\n",
    "print(report_stack)\n",
    "\n",
    "prediction_results(model=stacking_model)"
   ],
   "id": "67cbff68fcfa289",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Gradient boost with grid search",
   "id": "204a32361cc18fcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T06:44:24.742402Z",
     "start_time": "2024-10-12T06:43:11.446724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parameter grid for Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy: {accuracy_gb}\")\n",
    "\n",
    "# Classification report\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "report_gb\n",
    "\n",
    "prediction_results(model=best_gb_model)"
   ],
   "id": "e38f468071337a41",
   "execution_count": 73,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neural networks",
   "id": "9b2ef9370df1c003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())  # Add Batch Normalization for faster convergence\n",
    "model.add(Dropout(0.4))  # Increased dropout rate to prevent overfitting\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.0005)  # Lower learning rate for more fine-tuned updates\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping with increased patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "prediction_results(model=model)"
   ],
   "id": "167f246fae18379a",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3220602,
     "sourceId": 34377,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.229832,
   "end_time": "2023-11-15T07:14:47.675245",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-15T07:14:42.445413",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
